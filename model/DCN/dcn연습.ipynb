{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.Model):\n",
    "\n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self._model = model\n",
    "    self._logit_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[\n",
    "        tf.keras.metrics.RootMeanSquaredError(\"RMSE\")\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self._model(x)\n",
    "    return self._logit_layer(x)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    x, labels = features\n",
    "    scores = self(x)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=scores,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossnet = Model(tfrs.layers.dcn.Cross())\n",
    "deepnet = Model(\n",
    "    tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\401-14\\AppData\\Local\\Temp\\ipykernel_30936\\1382598649.py:12: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  new_song = pd.read_sql_query('SELECT * FROM new_song', connection)\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "host = '127.0.0.1'\n",
    "port = 3306\n",
    "user = 'root'\n",
    "password = 'acorn1234'\n",
    "database = 'music'\n",
    "\n",
    "connection = pymysql.connect(host=host, port=port, user=user, password=password, database=database)\n",
    "\n",
    "new_song = pd.read_sql_query('SELECT * FROM new_song', connection)\n",
    "\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# new_song 데이터프레임에서 'age', 'gender', 'like' 열을 각각 가져옵니다.\n",
    "age_column = new_song['age'].values\n",
    "gender_column = new_song['gender'].values\n",
    "score_column = new_song['sentiment_score'].values\n",
    "\n",
    "# age, gender, like 열을 합쳐서 train_x에 저장합니다.\n",
    "train_x = tf.stack([age_column, gender_column], axis=1)\n",
    "\n",
    "# like 열을 train_y에 저장합니다.\n",
    "train_y = score_column\n",
    "\n",
    "# train_x1,train_y1,test_x,test_y=train_test_split(train_x,train_y,test_size=0.2,random_state=42)\n",
    "# 학습 데이터셋 생성\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 0s 1ms/step - RMSE: 1.1845 - loss: 1.4032 - regularization_loss: 0.0000e+00 - total_loss: 1.4032\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 1ms/step - RMSE: 1.1412 - loss: 1.3123 - regularization_loss: 0.0000e+00 - total_loss: 1.3123\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 1ms/step - RMSE: 1.1361 - loss: 1.3001 - regularization_loss: 0.0000e+00 - total_loss: 1.3001\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 1ms/step - RMSE: 1.1341 - loss: 1.2953 - regularization_loss: 0.0000e+00 - total_loss: 1.2953\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 889us/step - RMSE: 1.1329 - loss: 1.2924 - regularization_loss: 0.0000e+00 - total_loss: 1.2924\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 889us/step - RMSE: 1.1319 - loss: 1.2901 - regularization_loss: 0.0000e+00 - total_loss: 1.2901\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 889us/step - RMSE: 1.1311 - loss: 1.2883 - regularization_loss: 0.0000e+00 - total_loss: 1.2883\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 889us/step - RMSE: 1.1303 - loss: 1.2866 - regularization_loss: 0.0000e+00 - total_loss: 1.2866\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 889us/step - RMSE: 1.1297 - loss: 1.2851 - regularization_loss: 0.0000e+00 - total_loss: 1.2851\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 889us/step - RMSE: 1.1291 - loss: 1.2837 - regularization_loss: 0.0000e+00 - total_loss: 1.2837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a67d2b2290>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32), y))\n",
    "\n",
    "crossnet.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate))\n",
    "crossnet.fit(train_dataset, epochs=epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 2.1617 - loss: 4.1703 - regularization_loss: 0.0000e+00 - total_loss: 4.1703\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1422 - loss: 1.3308 - regularization_loss: 0.0000e+00 - total_loss: 1.3308\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1189 - loss: 1.2616 - regularization_loss: 0.0000e+00 - total_loss: 1.2616\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1180 - loss: 1.2584 - regularization_loss: 0.0000e+00 - total_loss: 1.2584\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1176 - loss: 1.2561 - regularization_loss: 0.0000e+00 - total_loss: 1.2561\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1174 - loss: 1.2546 - regularization_loss: 0.0000e+00 - total_loss: 1.2546\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1172 - loss: 1.2538 - regularization_loss: 0.0000e+00 - total_loss: 1.2538\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1172 - loss: 1.2532 - regularization_loss: 0.0000e+00 - total_loss: 1.2532\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1170 - loss: 1.2526 - regularization_loss: 0.0000e+00 - total_loss: 1.2526\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 5ms/step - RMSE: 1.1170 - loss: 1.2522 - regularization_loss: 0.0000e+00 - total_loss: 1.2522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a67e482090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepnet.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate))\n",
    "deepnet.fit(train_dataset, epochs=epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "[[1.1869025]]\n"
     ]
    }
   ],
   "source": [
    "age_value=0\n",
    "gender_value=0\n",
    "\n",
    "# 예측하고자 하는 데이터를 input_data에 전처리하여 넣기\n",
    "input_data = tf.constant([[age_value, gender_value]], dtype=tf.float32)\n",
    "\n",
    "# deepnet 모델을 사용하여 예측 수행\n",
    "predictions = deepnet.predict(input_data)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.42173678]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[1.1869025]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[1.6086392]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "추천 음악 제목: 0    떠나보낼 준비해 둘걸 그랬어\n",
      "Name: song_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1. 입력 데이터 준비\n",
    "# 사용자의 나이와 성별 정보 (예시로 25세 여성)\n",
    "user_age = 0\n",
    "user_gender = 0  # 여성은 1, 남성은 0으로 지정\n",
    "\n",
    "# 2. 입력 데이터 전처리\n",
    "# 모델이 입력으로 받는 형태로 변환 (2차원 텐서로 변환)\n",
    "user_features = tf.constant([[user_age, user_gender]], dtype=tf.float32)\n",
    "\n",
    "# 3. DCN 모델 예측\n",
    "# crossnet과 deepnet 모델의 예측 값을 합치거나 결합하여 최종 예측을 생성 (여기서는 더해보겠습니다)\n",
    "crossnet_prediction = crossnet(user_features)\n",
    "deepnet_prediction = deepnet(user_features)\n",
    "\n",
    "# 두 모델의 예측 값을 더해서 최종 추천 점수 생성\n",
    "recommendation_score = crossnet_prediction + deepnet_prediction\n",
    "\n",
    "# 4. 음악 추천 결과 해석\n",
    "# 추천 점수가 가장 높은 음악을 추천으로 제공\n",
    "recommended_music_index = tf.argmax(recommendation_score, axis=1)\n",
    "recommended_music_title = new_song.iloc[recommended_music_index]['song_name']\n",
    "\n",
    "print(crossnet_prediction)\n",
    "print(deepnet_prediction)\n",
    "print(recommendation_score)\n",
    "print(recommended_music_index)\n",
    "\n",
    "\n",
    "print(\"추천 음악 제목:\", recommended_music_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자의 gender와 age 정보 입력\n",
    "user_gender = 0  # 여성은 1, 남성은 0으로 지정\n",
    "user_age = 2\n",
    "\n",
    "# DCN 모델 예측\n",
    "user_features = tf.constant([[user_age, user_gender]], dtype=tf.float32)\n",
    "crossnet_prediction = crossnet(user_features)\n",
    "deepnet_prediction = deepnet(user_features)\n",
    "\n",
    "# Annoy 모델과 DCN 모델 결과 조합\n",
    "def combine_recommendation_scores(crossnet_pred, deepnet_pred, annoy_scores):\n",
    "    # crossnet과 deepnet의 예측 값을 가중치로 사용하여 Annoy 모델의 추천 점수를 업데이트\n",
    "    combined_scores = crossnet_pred + deepnet_pred + annoy_scores\n",
    "    return combined_scores\n",
    "\n",
    "# 곡 추천 함수\n",
    "def recommend_songs_with_personalization(song_id, annoy_index, tfidf_matrix, crossnet_pred, deepnet_pred, num_neighbors=10):\n",
    "    # 주어진 곡 ID에 해당하는 가사 벡터를 가져옴\n",
    "    query_vector = tfidf_matrix[song_id]\n",
    "\n",
    "    # Annoy 모델을 사용하여 가장 유사한 이웃 곡들을 찾음\n",
    "    neighbors = annoy_index.get_nns_by_vector(query_vector, num_neighbors)\n",
    "\n",
    "    # 가장 유사한 이웃 곡들의 인덱스를 반환\n",
    "    similar_songs = np.array(neighbors)\n",
    "\n",
    "    # Annoy 모델에서 추천한 곡들의 점수를 계산\n",
    "    annoy_scores = tf.constant([tfidf_matrix_dense[i] for i in similar_songs], dtype=tf.float32)\n",
    "\n",
    "    # DCN 모델의 예측 결과를 이용하여 추천 점수를 업데이트\n",
    "    combined_scores = combine_recommendation_scores(crossnet_pred, deepnet_pred, annoy_scores)\n",
    "\n",
    "    # 추천 결과를 정렬하여 상위 곡들의 인덱스를 반환\n",
    "    top_recommended_songs = tf.argsort(combined_scores, direction='DESCENDING')[:num_neighbors]\n",
    "\n",
    "    return top_recommended_songs\n",
    "\n",
    "# 예시: id가 0인 곡과 유사한 가사를 가진 곡 추천 (개인화된 추천)\n",
    "similar_songs_personalized = recommend_songs_with_personalization(26, annoy_index, tfidf_matrix_dense, crossnet_prediction, deepnet_prediction)\n",
    "print(similar_songs_personalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_songs_personalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "mat = crossnet._model._dense.kernel\n",
    "features = [\"age\", \"gender\",\"like\"]\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "im = plt.matshow(np.abs(mat.numpy()), cmap=plt.cm.Blues)\n",
    "ax = plt.gca()\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(im, cax=cax)\n",
    "cax.tick_params(labelsize=10) \n",
    "_ = ax.set_xticklabels([''] + features, rotation=45, fontsize=10)\n",
    "_ = ax.set_yticklabels([''] + features, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# 데이터 준비\n",
    "# ... (데이터를 로드하고 전처리하는 코드)\n",
    "\n",
    "# 하이퍼파라미터 조합 설정\n",
    "layer_sizes = [[512, 256, 128]]\n",
    "epochs_list = [10,20,30,40,50]\n",
    "learning_rates = [0.1,0.2,0.3,0.4,0.5]\n",
    "\n",
    "# 파라미터 튜닝을 위한 Grid Search\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    # logits 형태의 예측값을 클래스로 변환\n",
    "    predicted_classes = tf.argmax(predictions, axis=1)\n",
    "    # 실제 레이블과 비교하여 일치하는 샘플의 비율 계산\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_classes, labels), tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "for layer_size in layer_sizes:\n",
    "    for epochs in epochs_list:\n",
    "        for learning_rate in learning_rates:\n",
    "            print(f\"Training model with layer_size={layer_size}, epochs={epochs}, learning_rate={learning_rate}\")\n",
    "\n",
    "            # 모델 생성\n",
    "            crossnet = Model(tfrs.layers.dcn.Cross())\n",
    "            deepnet = Model(\n",
    "                tf.keras.Sequential([\n",
    "                    tf.keras.layers.Dense(size, activation=\"relu\") for size in layer_size\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # 데이터 타입 변경\n",
    "            train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32), y))\n",
    "\n",
    "            # 모델 컴파일 및 학습\n",
    "            crossnet.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate))\n",
    "            crossnet.fit(train_dataset, epochs=epochs, verbose=False)\n",
    "\n",
    "            deepnet.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate))\n",
    "            deepnet.fit(train_dataset, epochs=epochs, verbose=False)\n",
    "\n",
    "            # 모델 평가\n",
    "            # ... (평가 데이터에 대해 모델을 평가하는 코드)\n",
    "            # 예를 들어, test_dataset를 사용하여 평가\n",
    "            crossnet_accuracy = compute_accuracy(crossnet.predict(train_dataset), train_y)\n",
    "            deepnet_accuracy = compute_accuracy(deepnet.predict(train_dataset), train_y)\n",
    "            \n",
    "            # 평가 결과를 기준으로 최적의 모델 선정\n",
    "            if crossnet_accuracy > best_accuracy:\n",
    "                best_accuracy = crossnet_accuracy\n",
    "                best_model = (layer_size, epochs, learning_rate)\n",
    "\n",
    "            if deepnet_accuracy > best_accuracy:\n",
    "                best_accuracy = deepnet_accuracy\n",
    "                best_model = (layer_size, epochs, learning_rate)\n",
    "\n",
    "# 최적의 파라미터 출력\n",
    "print(\"Best Model:\")\n",
    "print(f\"Layer Sizes: {best_model[0]}\")\n",
    "print(f\"Epochs: {best_model[1]}\")\n",
    "print(f\"Learning Rate: {best_model[2]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
